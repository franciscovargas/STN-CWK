\documentclass[11pt]{article}                                                                           
\renewcommand{\baselinestretch}{1.05}                                                                   
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx,listings,mathtools}                
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}                                                          
\usepackage{graphics}                                                                                   
\usepackage{color}                                                                                      
\definecolor{mygreen}{rgb}{0,0.6,0}                                                                     
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}} 
% \addtolength{\topmargin}{-1.0in}
\usepackage{ marvosym }
\usepackage{subcaption}
\usepackage{amsmath} 
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

% something NOT relevant to the usage of the package.
\usepackage{url,textcomp}
\setlength{\parindent}{0pt}
\setlength{\parskip}{18pt}
\title{\texttt{mcode.sty} Demo}
\author{Florian Knorn, \texttt{florian@knorn.org}}
\usepackage{listings}

 \begin{document}
 % $insert math here$ and \[inset math here \] represent the math environment
 % within the math environment we have:
        % \; - a thick space
        % \: - a medium space
        % \, - a thin space
        % \! - a negative thin space
 % indentation only holds within listings.
\title{Spectral Clustering On Similarity Based networks}
\author{s1235260}
% Q 2
% rmse_train = 0.0506
% rmse_test = 0.0503
% Q 3 
% test: 0.0502
% train : 0.0504
% Q 4  : 
% rmse_all_training = 0.0371
% rmset_all_test = 0.0456
% Q 5
% err3 = 0.0501
\maketitle
\section{Part 1}
\subsection{Question 1}
\subsubsection{Part a}
 The shades of gray range $G_{i} \in [0, 63], G_{i} \in \mathbb{Z}$ thus spanning 64 integer values over which we count the frequencies. If we were to construct a histogram which represents a distribution for the pixels they are already naturally discretized in 64 bins and this would be the ideal bin size. Any less would be loosing resolution in the way we picture the probability distribution since we have bigger bins.
 
 This is not the case here though since we are constructing a histogram for the standard deviation of the pixels per patch.
 
 Interpreting this standard deviation per patch as a random variable we know that the maximum value it can take is $ \frac{63}{2}$ and the smallest value it can take is $0$ thus we know it spans values in the range  $[0, 31.5]$ (one can prove that if $X \in [0,p]$ then $max(\sqrt{Var(X)})= \frac{p}{2}$. Given this information why not use $32$ bins ? Using $32$ bins give an extremely poor resolution and since the standard deviation can take rational values in between two integer numbers. By increasing the bin size to $64$ one is halving the range of values a bin takes and thus increasing the resolution of the histogram (when comparing these two one can note a significant improvement). 
 
 Another question would be why not use $64*2$ or even higher powers of two? My answer to this is that since the data set is limit we do not get such a high range of variances and increasing the number of bins too much will result in producing a noisy histogram (which happens at $64*4$) in terms of the resolution analogy used one could think of this as aliasing. In a sense $32$ is a reasonable value to start at $64$ is significantly better $128$ does not show much change and $128*2$ is extremely noisy thus $64$ seems like the most optimal candidate.
 
 Finally why powers of $2$ why double the number of bins from the range ? why not 70 or 50 ? The reason for this would be that by choosing a power of two (halving the bin size) one is more likely to even spread the standard deviations in to the two new sharper bins which I believe preserves more the shape of the underlying distribution.
 \begin{center}
 \includegraphics[scale=0.37]{hist1.jpg}
 \end{center}

\subsubsection{Part b }

A simple heuristic  would be ($N$ represents the number of of pixels in the patch):
\[
y(j) = \frac{1}{N} \sum_{i = 1}^{N }x(i)
\]
In words take the mean of the pixels in the patch to predict y.
\subsubsection{Part c}
\begin{lstlisting}
xtr = [xtr zeros(size(xtr,1),18)];
xtr_nf2 = [xtr zeros(size(xtr,1),18)];
flat = reshape(xtr(22,:),[35,30]).';
non_flat = reshape(xtr(154,:), [35, 30]).';
figure
colormap gray
imagesc(flat, [0,1])
figure
colormap gray
imagesc(non_flat, [0,1])
\end{lstlisting}

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
\centering
\caption{flat}
\includegraphics[scale=0.3]{flat.jpg}
\end{subfigure}%
\begin{subfigure}{.5 \textwidth}
\centering
\caption{not flat}
\includegraphics[scale=0.3]{not_flat.jpg}
\end{subfigure}
\end{figure}


\subsection{Question 2}
\subsubsection{Closed Form}

This form is derived as the analytical solution to minimizing the mean squared error function or maximizing the likelihood  s.t. $P(y | \underline{x}, \underline{w}) = \mathcal{N}(y | \mu(\underline{x}), \sigma^{2}(\underline{x}) )$ over all data points also yields the same result :
\[\underline{w} =\underbrace{\left( \left( \begin{array}{ccc}
x_{1, end} & x_{1, end -34} & 1 \\
x_{2, end} & x_{2, end -34} & 1 \\
&\vdots &\\
x_{N, end} & x_{N, end -34} & 1 \end{array} \right)^{T}
\left( \begin{array}{ccc}
x_{1, end} & x_{1, end -34} & 1 \\
x_{2, end} & x_{2, end -34} & 1 \\
&\vdots &\\
x_{N, end} & x_{N, end -34} & 1 \end{array} \right) \right) ^{-1} }_{(X_{}^{T}X_{})^{-1}} \overbrace{\left( \begin{array}{ccc}
x_{1, end} & x_{1, end -34} & 1 \\
x_{2, end} & x_{2, end -34} & 1 \\
&\vdots &\\
x_{N, end} & x_{N, end -34} & 1 \end{array} \right)^{T}}^{X_{}^T}
 \underbrace{\left( \begin{array}{c}
y_{1}  \\
y_{2}  \\
\vdots \\
y_{N}  \end{array} \right)}_{\underline{y} }\] 


%
%w =
%
%    0.4606
%    0.5241
%    0.0026
\subsubsection{3D Scatter Plots}


\includegraphics[scale=0.4]{scat31.jpg}
\includegraphics[scale=0.4]{scat32.jpg}
\newline\newline
In general one can observe a linear (planar) structure of pixel to be predicted in terms of the adjacent pixels. There are indeed some outliers . Further down in the plane fitted plots  (with more perspective) one can observe how at angles the scatter data does indeed lie on a straight line (with the exception of outliers).
\newline
\subsubsection{Linear Regressor}
\begin{lstlisting}
X = [xtr_nf(:,end) xtr_nf(:,end-34) transpose(ones(1,size(xtr_nf(:,end),1)))];
Xt = [xte_nf(:,end) xte_nf(:,end-34) transpose(ones(1,size(xte_nf(:,end),1)))];
w = inv(transpose(X) * X) * transpose(X) * ytr_nf;

W = repmat(transpose(w),size(ytr_nf,1),1);
y = X * w ;
y1 = Xt * w;

rmse_train = sqrt((1/size(ytr_nf,1))*(transpose(ytr_nf - y)) * (ytr_nf - y));
rmse_test = sqrt((1/size(yte_nf,1))*(transpose(yte_nf - y1)) * (yte_nf - y1));

figure,
[dim1, dim2 ] = meshgrid(0:0.01:1,0:0.01:1);
ysurf = [[dim1(:), dim2(:)], ones(numel(dim1),1)]*w;
plot3(xte_nf(:,end),xte_nf(:,end-34),yte_nf(:), 'ro')
hold on
surf(dim1, dim2, reshape(ysurf, size(dim1)))
xlabel('Adjacent Pixel end') % x-axis label
ylabel('Adjecent Pixel end - 34') % y-axis label
zlabel('Pixel to Predict') % y-axis label
hold off

\end{lstlisting}
The results where the following:
\begin{center}
\begin{tabular}{|c|c|}
\hline
 rmse\_train & rmse\_test \\
 \hline
0.0506 & 0.0503 \\
\hline
\end{tabular}
\end{center}
\[
\underline{w} =
\left (
\begin{array}{c}
    0.4606 \\
   0.5241 \\
    0.0026\end{array} \right )
\]
 
\includegraphics[scale=0.31]{plane1.jpg}
\includegraphics[scale=0.31]{plane2.jpg}

Inspecting the graphs one (specially the second one) one can observe that the plane traverses the through the data set as one would expect a line of best fit to traverse through a 2D case regression. The errors also support the nice graphical performance of the regressor since they are very small.


\subsection{Question 3}


% test: 0.0502
% train : 0.0504
\begin{center}
\begin{tabular}{|c|c|}
\hline
 rmse\_train & rmse\_test \\
 \hline
0.0500 & 0.0502 \\
\hline
\end{tabular}
\end{center}
The snippet which generates the plots via cross-validation is the following: 
\begin{lstlisting}
load('imgregdata.mat')
X = xtr_nf;
Xt_all = xte_nf;
X_cross = [X(:,[(end -34) end]) ones(17261,1)];
X_cross_t = [Xt_all(:,[(end -34) end]) ones(7309,1)];
basis_nums = [5, 10, 15, 20, 25, 30];
d = []
ms = []
to_mod = []
for j=1:1
ms = []
for i=basis_nums
    nin = 3;
    nhidden = i;
    nout = 1;

    options = foptions;
    options(1) = 1;
    options(14) = 5;
    predfun = @(XTRAIN, ytrain, XTEST)(rbffwd(rbftrain(rbf(nin, i, nout, 'gaussian'),...
                                                       options, ...
                                                       XTRAIN, ...
                                                       ytrain), XTEST));
    a = crossval('mse', X_cross, ytr_nf,'predfun',predfun);
    d = [d sqrt(a)];
    ms = [ms a];
end
[n, ii] = min(ms);
to_mod = [to_mod basis_nums(ii)];
end

plot(basis_nums, d)
\end{lstlisting}

Which yields the following plot:

\includegraphics[scale=0.4]{10.jpg}

Every time the model is ran the plot is slightly different and sometimes the best number of basis functions is 10 sometimes not.
This is because the centers of the RBF.  The reason behind this is that the starting points of the GMMs which define the centres of our radial basis units are picked randomly in each run. After several runs 10 hidden units seemed to be the best performing number. 


This model performs well since there is a radial like (not a very strong one) nature in the data as one can observe when visualizing the two adjacent pixels (observe the ellipsoidal like cluster close to 0). When the data is radial it becomes linear when projected in the feature space spanned by the radial basis function.

It is hard to comment on why the number of basis functions used changes the performance. Since it is a neural network with a radial basis transfer function in the hidden layer it is tempting to interpret each hidden unit generating a feature from the two dimensions in the data but an alternative interpretation is also that it is fitting different spheres (radial basis functions over the data) and a specific number may be the correct number of spherical clusters that are in the data. Sadly it is hard to justify visually why 10 is indeed a good value. 

Based on the last sentence I would argue that 10 seems a bit random and thus radial basis are not being exploited to the fullest because as mentioned previously the radial nature is there but its not very strong.


\subsection{Question 4}

The results over the new model are the following
\begin{center}
\begin{tabular}{|c|c|}
\hline
 rmse\_train & rmse\_test \\
 \hline
0.0371 & 0.0456 \\
\hline
\end{tabular}
\end{center}
As expected the performance over this model is notably higher. This is used to the fact that more information is used to predict the pixel from the image patch. For reasons such as noise the first two pixels may not be a good enough pixels to predict in some cases. As we can observe from our scatter plots the adjacent pixels do indeed exhibit some outliers and thus using more pixels may help to to compensate these. Moreover looking at pixels really far away from $\underline{y}$ may not be ideal since they could not be related nonetheless I believe the reason that this does not cause such a negative impact may be because the MLE solution weights such attributes less and the relatively substantial size of our training set helps with this.


\subsection{Question 5}
%
%rmse_NNsuball_te_2016 =
%
%    0.0473
%
%>> rmse_NNsuball_tr_2016
%
%rmse_NNsuball_tr_2016 =
%
%    0.0333


\subsubsection{Part a}

The results for this section are the following:
\begin{center}

\begin{tabular}{|c|c|}
\hline
 rmse\_train & rmse\_test \\
 \hline
0.0333 & 0.0473 \\
\hline
\end{tabular}
\end{center}

This model seems to over-fit  when  comparing to the radial basis
network. This claim is justified because the training error of the neural network in this part of the assignment is smaller than that of the RBF network whilst the test error of the RBF network is smaller than that of this neural network. This over fit may happen due to the extra added layer of complexity in our NN since it trains a total of $10320$ weights due to its overall 10 hidden units (This gives a lot of extra parameters for the model to learn noise from the data and interpret it as a useful trait). Nonetheless the overall performance can still be considered as a good performance in comparison to the linear regression models.

%preds_te1 =
%
%    0.0508    0.0502    0.0515    0.0520    0.0526
%
%>> preds_tr1
%
%preds_tr1 =
%
%    0.0492    0.0474    0.0485    0.0480    0.0489
\subsubsection{Part b}

The results for this section are the following:
\begin{center}
\begin{tabular}{|c|c|}
\hline
 rmse\_test & rmse\_train \\
 \hline
0.0508 &  0.0492 \\
0.0502  & 0.0474 \\
0.0515 & 0.0485 \\
0.0520 & 0.0480 \\
0.0526 & 0.0489 \\
\hline
\end{tabular}
\end{center}

From my understanding the purpose of using different random seeds is to look at the overall behaviour across each seeding. Overall I can conclude that this model does not over fit since its performance over the training set relative to other models is not better whilst performing worse on the training set.  Why this happens is that we may have removed the outlier like data points which we were fitting when reducing the size of the training set.

The downside of having a smaller training set is that now our training set may not be as representative enough of the underlying model behind the data and thus our performances over the test set are not as high as when using the full training set.

\subsection{Enhancements and General Comments}

\subsubsection{Remarks on the Data Set}

  Observing some of the 3D plots it is quite obvious that there are outliers in the data set. An alternate error metric which would have been useful to consider would have been the mean absolute error which is less sensitive to outliers. I suspects the MAE would have been relatively smaller than the RMSE since it wont be affected much by the outliers and this would imply that there are outliers in our data.
  
\subsubsection{Comment on the Units of the RMSE}

I have not given a clear justification as to why the error values are small. This is because they are in the same units as the pixel values which span from 0 to 1 and thus most errors being of the scale of $0.0n$ one can consider this to be relatively small given that $y$ is in the range 0 to 1.

\subsubsection{Overall Model Comparison}

One thing that is noticeable is that there is not really a massive improvement in between the models. My hypothesis as to why this is the case is that the data is already quite linearly distributed and thus linear regression is hard to significantly outperform. This being said the outliers in the dataset (which need not be error) are hard for a hyperplane to predict as one can observe in the plane fit images and this may be why one wants to use a more general model such as a neural network to capture these patterns in the image. Sadly using a neural network seems to come with the price of over fitting which may mean that some of these extreme values are not worth learning.



\section{Part 2}

% 1b)
% acc = 0.9083 +/- 0.0072 , mean log like = - 0.2916 +/- 0.0068

% train acc 0.8322 +/- 0.004634846433419
% sigtrain log  0.0083
% log train -0.444087219265299

% 1 c)
% acc = 0.784615384615385 +/-0.0102    log like = 2.599262269814561 +/ - 0.174631837280862


% 2b 
% acco =  0.913846153846154 +/- 0.0070;
% log = -0.201261292523567   sigmo =0.009469188201563 

\subsection{Question 1}

\subsubsection{Part b}
Results:

\begin{center}
\begin{tabular}{|c|c|}
\hline
Accuracy\_test & standard error \\
 \hline
0.9071 & 0.0072 \\
\hline
\end{tabular}
\begin{tabular}{|c|c|}
\hline
Mean Log Likelihood\_test & standard error \\
 \hline
- 0.2931 & 0.0085 \\
\hline
\end{tabular}
\end{center}



% train acc 0.8322 +/- 0.004634846433419
% sigtrain log  0.0083
% log train -0.444087219265299


\begin{center}
\begin{tabular}{|c|c|}
\hline
Accuracy\_train & standard error \\
 \hline
0.8335 & 0.0046 \\
\hline
\end{tabular}
\begin{tabular}{|c|c|}
\hline
Mean Log Likelihood\_train & standard error \\
 \hline
- 0.4398 & 0.0081 \\
\hline
\end{tabular}
\end{center}
I am interpreting the mean log likelihood  as a measure of how certain one is when predicting the correct test labels. 

As one can observe the training results are better in both accuracy and standard error than the test set. This is due to the fact that the training set is known to contain errors, thus creating a high level of insecurity in our predictions making it hard to predict the erroneous training labels. Nonetheless the confidence in our training predictions is still better than the baseline and thus not as bad as flipping a coin.

The mean log likelihood of the baseline is $-0.6931 \pm 0$  with an accuracy of $0.4283 \pm 0.0123$  which overall performs significantly worse than our MLE based classifier.

\begin{lstlisting}
load('text_data.mat');
%%%%%%%%%%%%% 1 a %%%%%%%%%%%%%%%%%%%%%%%%%%%
X = [ x_train transpose(ones(1,size(x_train,1)))];
Xc = X(1:100,:);
Xtst = [ x_test transpose(ones(1,size(x_test,1)))];
ww = transpose(zeros(1, 101));
mw = minimize(ww, @neg_loglike, 500, X, y_train);
%%%%%%%%%%%%%%% 1 b %%%%%%%%%%%%%%%%%%%%%%%%%%
yb =1 ;
% Probabilities
pr = prob(mw,Xtst, yb);
% Predictions
y_pred = single(pr > 0.5);
y_pred(~y_pred) = -1;
%Accuracy
y_acc2 = sum(y_test == y_pred) / size(y_test,1);
ysig2 = std(y_test == y_pred) / sqrt(1625);
% Mean log likelihood
ml =  (1/size(y_test,1)) * lr_loglike(mw, Xtst, y_test);
% Eror bars
sig = error_bars(mw,Xtst,y_test);
% For train :
prt = prob(mw,X, yb);
% Predictions
y_pred_train = single(prt > 0.5);
y_pred_train(~y_pred_train) = -1;
y_acc_train = sum(y_train == y_pred_train) / size(y_train,1);
err_train = std(y_train == y_pred_train) / sqrt(size(y_train,1));
mltrain =  (1/size(y_train,1)) * lr_loglike(mw, X, y_train);
sigtrain = error_bars(mw,X,y_train);
%Baseline computations
bassline = ones(1, size(pr,1)) - 0.5;
pred_bass = (single(bassline > 0.5))';
pred_bass(~pred_bass) = -1;
y_accbass = sum(y_test == pred_bass) / size(y_test,1);
ysigbass = std(y_test == pred_bass) / sqrt(1625);
\end{lstlisting}

\begin{lstlisting}
%error_bars routine
function sig = error_bars(ww, xx, yy)

yy = (yy==1)*2 - 1;
sigmas = 1./(1 + exp(-yy.*(xx*ww))); % Nx1
sig = sqrt(var(log(sigmas)) / size(sigmas,1) );
\end{lstlisting}



\subsubsection{Part c}
Results:

% 1 c)
% acc = 0.784615384615385 +/-0.0102    log like = 2.599262269814561 +/ - 0.174631837280862


\begin{center}
\begin{tabular}{|c|c|}
\hline
Accuracy\_test & standard error \\
 \hline
0.7846 & 0.0102  \\
\hline
\end{tabular}
\begin{tabular}{|c|c|}
\hline
Mean Log Likelihood\_test & standard error \\
 \hline
- 5.7835 & 0.0105 \\
\hline
\end{tabular}
\end{center}

All results using the mini training set are significantly worse than when using the training set both the confidence in which we are predicting the test labels and the accuracy.  This is because the training set is not representative enough of the data (being such a small sample) and thus it cannot learn enough relevant patterns to perform well when classifying a test set. This exemplifies  why generally we need a well sized training set.

\begin{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 1 c %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
mw2 = minimize(ww, @neg_loglike, 500, Xc, y_train(1:100));
pr2 = prob(mw2,Xtst, yb);
% Predictions
y_predc = single(pr2 > 0.5);
y_predc(~y_predc) = -1;
% Mean log likelihood
mlc =  (1/size(y_test,1)) * lr_loglike(mw2, Xtst, y_test);
y_acc2c = sum(y_test == y_predc) / size(y_test,1);
ysig2c = std(y_test == y_predc) / sqrt(1625);
\end{lstlisting}


\subsection{Question 2}
\subsubsection{Derivation part 2a}

I have chosen to do this by introducing a dummy variable (via marginalizing y):

\[
P(y| \underline{w}, \underline{x}) = \sum_{s_{n}\in \{ 0,1\}}(P(y,s  = s_{n} | \underline{w}, \underline{x}))
\]

\[
P(y| \underline{w}, \underline{x}) = \sum_{s_{n}\in \{ 0,1\}}P(y | \underline{w}, \underline{x},s = s_{n})P( s= s_{n}|\underline{w}, \underline{x}) 
\]

 Since  $s$ Does not depend on either $\underline{w}$ or $\underline{x}$ thus $P( s= s_{n}|\underline{w}, \underline{x})  = P( s= s_{n}) $

\[
P(y| \underline{w}, \underline{x}) = P(y | \underline{w}, \underline{x},s = 0)P(s = 0)  + P(y | \underline{w}, \underline{x},s = 1)P(s = 1)
\]
\newline
 We have that y is drawn from a sigmoid when  $s=0$
 thus $P(y | \underline{w}, \underline{x},s = 0) = \sigma( \underline{w}^{T} \underline{x})$  and for the case where $s=1$ we have that its uniformly distributed over 2 discrete value which yields $P(y | \underline{w}, \underline{x},s = 1) = 0.5$:
 
 
 \[
P(y| \underline{w}, \underline{x}) = ( 1 - \epsilon)\sigma( y\underline{w}^{T} \underline{x})  + \frac{\epsilon}{2}
\]

Since $\epsilon$ is now a parameter we can augment it in our parameter list: 
 
 \[
P(y| \underline{w}, \underline{x}, \epsilon) = ( 1 - \epsilon)\sigma(y \underline{w}^{T} \underline{x})  + \frac{\epsilon}{2}
\]

Differentiating this we obtain:
$$\nabla_{\underline{w}} \mathcal{L}(\underline{w},\epsilon) = \sum_{n=1}^{N}\frac{(1 - \sigma(y^{(n)} \underline{w}^\top \underline{x}^{(n)})) \sigma(y^{(n)} \underline{w}^\top \underline{x}^{(n)}) (1 - \epsilon) y^{(n)}  \underline{x}^{(n)}}{(1 - \epsilon) \sigma(y^{(n)} \underline{w}^\top \underline{x}^{(n)}) + \epsilon/2}$$

$$ \frac{\partial \mathcal{L}}{\partial \epsilon} = \sum_{n=1}^{N} \frac{1/2 - \sigma(y^{(n)} \underline{w}^\top \underline{x}^{(n)})}{(1 - \epsilon) \sigma(y^{(n)} \underline{w}^\top \underline{x}^{(n)}) + \epsilon/2}$$


For the next (finite difference approximation) part I used the checkgrads routine with an $h=10^{-5}$ yielding a max difference of 3.3896e-07 and the checkgrads routine returned 2.1959e-10 as its final output. As one can observe both these numbers are indeed $O(h^{2})$.
\begin{lstlisting}
function [f, df] = delt_log(ww,xx, yy)
e = ww(end);
yy = (yy==1)*2 - 1;
sigmas = 1./(1 + exp(-yy.*(xx*ww(1:(end-1) ))));
f = sum(log((1 -e)*sigmas + 0.5*e));
probs = ( ( 1./((1-e).*sigmas +0.5*e) ));
epsilon = sum( probs .*(0.5 - sigmas));
w = -(( probs .* ((1 - e)*sigmas.*(1 - sigmas).*yy) )' * xx)';
if nargout > 1
df = [w' epsilon]';
end

\end{lstlisting}
\begin{lstlisting}
yo =  checkgrad(@delt_log, ones(size(ww,1) + 1, 1), 1e-5, X, y_train );
\end{lstlisting}
\subsubsection{Part 2b}

% 2b 
% acco =  0.913846153846154 +/- 0.0070;
% log = -0.201261292523567   sigmo =0.009469188201563 
%     0.2123

Results:
\begin{center}
\begin{tabular}{|c|c|}
\hline
Accuracy\_test & standard error \\
 \hline
0.9138 & 0.0070  \\
\hline
\end{tabular}
\begin{tabular}{|c|c|}
\hline
Mean Log Likelihood\_test & standard error \\
 \hline
-0.3893 & 0.0530 \\
\hline
\end{tabular}
\end{center}

\[
\epsilon = \sigma(a) = 0.2123
\]

Epsilon is relatively and this is why I believe the confidence level when predicting the  test label does not go down so dramatically. Nonetheless it does decrease notably and as mentioned in the lectures adding extra layers of complexity to a model is not guaranteed to make them improve in all senses. Yes the model performs better in the accuracy sense but with a lot less confidence in these predictions. I believe that this performance may be explained by the $\epsilon$ parameter fitting uncertainty in the learning process since it models the mislabling process.

\begin{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2 b %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
we = minimize(zeros(size(ww,1)+1,1), @delt_log_a, 500 , X, y_train); 

yb2 =1 ;
% Probabilities
pr2 = prob(we(1: end -1),Xtst, yb);
% Predictions
y_pred2 = single(pr2 > 0.5);
y_pred2(~y_pred2) = -1;
yb =1 ;
% mean log like
ml2b =  (1/size(y_test,1)) * lr_loglike(we(1:end -1), Xtst, y_test);
sig2b = error_bars(we(1:end-1),Xtst,y_test);
y_acc2b = sum(y_test == y_pred2) / size(y_test,1);
ysig2b = std(y_test == y_pred2) / sqrt(1625);
1/(1+exp(-we(end)))
\end{lstlisting}

\begin{lstlisting}
function [f, df] = delt_log_a(ww,xx, yy)
sigmaA = 1./(1 + exp(-ww(end)));
sigmas = 1./( 1 + exp(-yy.*(xx*ww(1:(end-1)))) );
f = -sum(log((1 -sigmaA)*sigmas + 0.5*sigmaA));
probs =  1./((1-sigmaA).*sigmas +0.5*sigmaA) ;
depsilon = -sum( probs .* sigmaA.*(1-sigmaA).*(0.5 - sigmas));
dw = -(( probs .* ((1 - sigmaA).*sigmas.*(1 - sigmas).*yy) )' * xx)';
if nargout > 1
    df = [dw' depsilon]';
%size(epsilon)
end
\end{lstlisting}
\subsection{Question 3}

% checkgrads min diff
% perturbation 10^-5
% 3.3896e-07
% check grads reported deviation
% 2.1959e-10

\subsubsection{Part a}

%MCMC acc = 0.9151 +/- 0.0069

%MCMC loglike = -0.1890 +/- 0.01308

The likelihood is defined between 0 and 1 thus the maximum value the log likelihood can take is  0. That is if all predictions are correct and with 100\% confidence. 

For $\underline{w} = \underline{0}$ we have :

\[
P(y| \underline{0}, \underline{x}, \epsilon) = (1- \epsilon)\underbrace{\sigma(0)}_{\frac{1}{2}} + \frac{\epsilon}{2} = \frac{1}{2} - \frac{\epsilon}{2} + \frac{\epsilon}{2} = 0.5
\]


Inspecting $log (P( \epsilon, \underline{w}, log (\lambda) | data) )= \mathcal{L}(\underline{w}, \epsilon) - \lambda \underline{w}^{T} \underline{w} +
\frac{D}{2}log(\lambda) + const$  one can observe  $\lambda \underline{w}^{T} \underline{w}$ is the only negative term and thus getting rid of this term will ensure we get the highest possible contribution from the weights and the likelihood just becomes one half which is a constant that does not mean much when taking the limit as $\lambda$ goes to $\infty$  since the term $log(\lambda)$ also goes to infinity.

\subsubsection{Part b}

The following graph shows us the relationship in between $\epsilon$  and $log(\lambda)$  as one can observe there is a negative like correlation in between these two random variables
due to the ellipsoidal nature it exhibits. Another observation I think worth of highlighting (this is why I introduced the color mapping relative to the iteration of the sample) is that samples that were in the last iteration are all more converging to the center of the ellipse which I interpret as the samples converging more to the real underlying joint distribution in between $log(\lambda)$ and $\epsilon$.  The previous statement could justify using a higher burn out period. 

\includegraphics[scale=0.51]{scatter_fin.jpg}


Observing the range of $\epsilon$ in this period one can see that the concentration is in between 0.19 and 0.215 which seems to suggest that the values are indeed reasonable and agree with the earlier part of this task.

Another comment is that as the error increases $\lambda$ decreases which means that the distribution from which $\underline{w}$ is drawn has a higher variance and thus we can sample weights which are all relatively far away from each other and not necessarily in agreement with each other prediction wise  thus the confidence over our predictions might be quite low. This interpretation would agree with the lower log likelihood obtained in the question where we fitted $\epsilon$.

\begin{lstlisting}
function val = samp_log(ww, xx,yy)
%size(ww(:,1:(size(ww,1))-2))
sigmas = 1./(1 + exp(-yy.*(xx*ww(1:end-2) ))); % Nx1
e = (1- ww(end -1))*sigmas + 0.5*ww(end -1);
l = sum(log(e));
val = l - exp(ww(end)) * transpose(ww)*ww  + 0.5*(size(ww,1) -2)*(ww(end));
if(ww(end-1) < 0 )
    val = - inf;
end
if(ww(end-1) > 1)
    val = - inf;
end

\end{lstlisting}

\begin{lstlisting}
ws  =  slice_sample(1000 , 10 , @samp_log,  ones(size(ww,1) + 2, 1)  - 0.5, 1, 1 , X, y_train);
yy = ones(1625,1);
c = linspace(1,1000,1000);
scatter(ws(end, :) , ws(end-1,:), [] , c);
\end{lstlisting}
\subsubsection{Part c}

%MCMC acc = 0.9151 +/- 0.0069

%MCMC loglike = -0.1890 +/- 0.01308
The samples drawn by slice sampling are used to approximate the following posterior:

\[
P(y|\underline{x}, \underline{w}, \mathcal{D}) = \int_{-\infty}^{\infty}P(y|\underline{w}, \underline{x})P(\underline{w}| \mathcal{D} )d\underline{w}
\]

Which can be approximated with our 1000 samples by:

\[
P(y|\underline{x}, \underline{w}, \mathcal{D}) \approx \frac{1}{1000}\sum_{i=1}^{1000}P(y|\underline{w}_{i}, \underline{x})
\]

Which is how we use our samples $\underline{w}_{i}$ to predict a label.

Results:
\begin{center}
\begin{tabular}{|c|c|}
\hline
Accuracy\_test & standard error \\
 \hline
0.9151 &0.0069 \\
\hline
\end{tabular}
\begin{tabular}{|c|c|}
\hline
Mean Log Likelihood\_test & standard error \\
 \hline
-0.1890& 0.01308 \\
\hline
\end{tabular}
\end{center}
Overall these results yield the best performance in both confidence over correct predictions and accuracy.

\begin{lstlisting}
%prediciton
pr3c = transpose(mean(1./(1 + exp(-(ws(1:(end-2),:)' * Xtst' )))));
y_pred3 = single(pr3c > 0.5);
y_pred3(~y_pred3) = -1;
%Accuracy
y_acc3 = sum(y_test == y_pred3) / size(y_test,1);
ysig3 = std(y_test == y_pred3) / sqrt(1625);
% Log 
ry = repmat(y_test, 1, 1000);
size(ry)
size((ws(1:(end-2),:)' * Xtst').')
t =  mean(log(mean(1./(1 + exp(-transpose((ws(1:(end-2),:)' * Xtst').' .* ry) )) )));
t_dev =  std(log(mean(1./(1 + exp(-transpose((ws(1:(end-2),:)' * Xtst').' .* ry) )) )))/ sqrt(1625);
\end{lstlisting}


\end{document}
